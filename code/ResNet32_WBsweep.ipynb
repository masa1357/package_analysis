{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## signate 画像分類コンペ(2クラス)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# インポート\n",
    "import glob\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import wandb\n",
    "import yaml\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初期処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = 'C:\\\\Users\\\\zigza\\\\GitFile\\\\signate\\\\package_analysis\\\\'\n",
    "DATA_PATH = BASE_PATH + 'datasets\\\\'\n",
    "TRAIN_PATH = DATA_PATH + 'train\\\\'\n",
    "TEST_PATH = DATA_PATH + 'test\\\\'\n",
    "OUT_PATH = BASE_PATH + 'out\\\\'\n",
    "# C:\\\\Users\\\\zigza\\\\GitFile\\\\signate\\\\package_analysis\\\\train.csv\n",
    "# C:\\Users\\zigza\\GitFile\\signate\\package_analysis\\datasets\\train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = '/home/masa1357/Dockerdata/gitfile/package_analysis/'\n",
    "DATA_PATH = BASE_PATH + 'datasets/'\n",
    "TRAIN_PATH = DATA_PATH + 'train/'\n",
    "TEST_PATH = DATA_PATH + 'test/'\n",
    "OUT_PATH = BASE_PATH + 'out/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_PATH + 'train.csv')\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seedの固定\n",
    "def fix_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "SEED = 0\n",
    "fix_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, file_list, transform=None):\n",
    "        self.file_list = file_list\n",
    "        self.transform = transform\n",
    "        self.img_path = TRAIN_PATH\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 画像を読みこんで、指定の方法でtransform\n",
    "        img_name = os.path.join(self.img_path, self.file_list.iloc[index,0])\n",
    "        img = Image.open(img_name)\n",
    "        img_transformed = self.transform(img)\n",
    "        label = int(self.file_list.iloc[index,1])\n",
    "\n",
    "        return img_transformed, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateDataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self, train_df, val_df, test_df, img_size=224,\n",
    "                 mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225),\n",
    "                 batch_size=16):\n",
    "        super().__init__()\n",
    "        self.train_df = train_df\n",
    "        self.val_df = val_df\n",
    "        self.test_df = test_df\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # train時、val/test時の前処理をそれぞれ定義\n",
    "        self.train_transforms = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(img_size, scale=(0.5, 1.0)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean, std)\n",
    "        ])\n",
    "\n",
    "        self.val_test_transforms = transforms.Compose([\n",
    "            transforms.Resize(img_size),\n",
    "            transforms.CenterCrop(img_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean, std)\n",
    "        ])\n",
    "\n",
    "    # データのダウンロードなどを行う場合は定義、今回は不要\n",
    "    def prepare_data(self):\n",
    "        pass\n",
    "\n",
    "    # Trainer.fit()ではtrain/valのDatasetを、Trainer.test()ではtestのDatasetを生成\n",
    "    def setup(self, stage=None):\n",
    "        if stage == 'fit' or stage is None:\n",
    "            self.train_dataset = MyDataset(self.train_df, self.train_transforms)\n",
    "            self.val_dataset = MyDataset(self.val_df, self.val_test_transforms)\n",
    "\n",
    "        if stage == 'test' or stage is None:\n",
    "            self.test_dataset = MyDataset(self.test_df, self.val_test_transforms)\n",
    "\n",
    "    # こちらもTrainer.fit()ではtrain/valのDataLoaderを、Trainer.test()ではtestのDataLoaderを生成\n",
    "    # trainはshuffleあり、val/testはshuffleなし\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seedを固定\n",
    "fix_seed(SEED)\n",
    "\n",
    "#train_df内のデータを7:2:1の割合でval_df,test_dfに分割\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.3, shuffle=True, random_state=SEED)\n",
    "val_df, test_df = train_test_split(val_df, test_size=0.33, shuffle=True, random_state=SEED)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassifier(pl.LightningModule):\n",
    "    def __init__(self, model_name, n_classes, lr=0.0001, criterion=torch.nn.CrossEntropyLoss()):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # timmで学習済みモデルをダウンロードし、classifier部分を付替え\n",
    "        # n_classesにはラベルの件数を渡す（今回はアリとハチの2つなので2）\n",
    "        self.model = models.resnet34(pretrained=True)\n",
    "        self.model.fc = nn.Linear(self.model.fc.in_features, n_classes)\n",
    "\n",
    "        self.lr = lr\n",
    "        self.criterion = criterion\n",
    "        self.outputs = []\n",
    "        \n",
    "        # net属性としてmodelをエイリアス化\n",
    "        self.net = self.model\n",
    "\n",
    "    # 順伝搬\n",
    "    def forward(self, imgs, labels=None):\n",
    "        preds = self.model(imgs)\n",
    "        loss = 0\n",
    "        if labels is not None:\n",
    "            loss = self.criterion(preds, labels)\n",
    "        return loss, preds\n",
    "\n",
    "    # trainのミニバッチに対して行う処理\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        imgs, labels = batch\n",
    "        loss, preds = self.forward(imgs=imgs, labels=labels)\n",
    "        self.log(f\"train_loss\", loss, on_step=True, on_epoch=True,prog_bar=True, logger=True)\n",
    "        return {'loss': loss, 'batch_preds': preds.detach(), 'batch_labels': labels.detach()}\n",
    "\n",
    "    # validation、testでもtrain_stepと同じ処理を行う\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        result = self.training_step(batch, batch_idx)\n",
    "        self.outputs.append(result)\n",
    "        self.log(f\"val_loss\", result['loss'], logger=True)\n",
    "        return result\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        result = self.training_step(batch, batch_idx)\n",
    "        self.log(f\"test_loss\", result['loss'], logger=True)\n",
    "        return result\n",
    "\n",
    "    # epoch終了時にvalidationのlossとaccuracyを記録\n",
    "    def on_validation_epoch_end(self):\n",
    "\n",
    "        # loss計算\n",
    "        epoch_preds = torch.cat([x['batch_preds'] for x in self.outputs])\n",
    "        epoch_labels = torch.cat([x['batch_labels'] for x in self.outputs])\n",
    "        epoch_loss = self.criterion(epoch_preds, epoch_labels)\n",
    "        self.log(f\"val_loss\", epoch_loss, logger=True)\n",
    "\n",
    "        # accuracy計算\n",
    "        num_correct = (epoch_preds.argmax(dim=1) == epoch_labels).sum().item()\n",
    "        epoch_accuracy = num_correct / len(epoch_labels)\n",
    "        self.log(f\"val_accuracy\", epoch_accuracy, logger=True)\n",
    "        self.outputs = []\n",
    "\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        ret = self.validation_epoch_end(self.outputs, \"test\")\n",
    "        self.outputs = []\n",
    "        return ret\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(lr=self.lr, params=self.model.parameters())\n",
    "        # schedulerの設定（どっちか）\n",
    "        #scheduler = {'scheduler': optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.2)}\n",
    "        warmup_steps = int(0.1 * self.trainer.max_steps)  # warmupのステップ数を全ステップの10%に設定\n",
    "        scheduler = {\n",
    "            'scheduler': get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=self.trainer.max_steps),\n",
    "            'interval': 'step',\n",
    "            'frequency': 1\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    #EarlyStoppingの設定\n",
    "    # 3epochで'val_loss'が0.05以上減少しなければ学習をストップ\n",
    "    early_stop_callback = EarlyStopping(\n",
    "        monitor='val_loss', min_delta=0.05, patience=3, mode='min')\n",
    "    \n",
    "    # モデルの保存先\n",
    "    # epoch数に応じて、「epoch=0.ckpt」のような形で保存\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=OUT_PATH + 'model_checkpoints/',\n",
    "        filename='{epoch}', monitor='val_loss', mode='min', verbose=True)\n",
    "    \n",
    "    \n",
    "    wandb.init(project=\"sweep_package_analysis\")\n",
    "    config=wandb.config\n",
    "    wandb_logger = WandbLogger()\n",
    "    model = ImageClassifier(model_name=\"ResNet34\", n_classes=2)\n",
    "\n",
    "    wandb_logger.watch(model.net)\n",
    "    \n",
    "    # インスタンスを作成\n",
    "    data_module = CreateDataModule(train_df,val_df,test_df ,batch_size=64)\n",
    "    \n",
    "    trainer = pl.Trainer(\n",
    "        accelerator='gpu',\n",
    "        devices=1,\n",
    "        max_epochs=30, \n",
    "        logger=wandb_logger,\n",
    "        callbacks=[checkpoint_callback, early_stop_callback],\n",
    "        log_every_n_steps=10\n",
    "        )\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    trainer.fit(model, data_module)\n",
    "    trainer.test(data_module.test_dataloader(),ckpt_path=checkpoint_callback.best_model_path)\n",
    "    #,ckpt_path=checkpoint_callback.best_model_path\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config_sweep_ResNet34.yaml', 'r') as file:\n",
    "    sweep_config = yaml.safe_load(file)\n",
    "print(sweep_config)\n",
    "sweep_id=wandb.sweep(sweep_config, project=\"Efficient_sweep\")\n",
    "wandb.agent(sweep_id=sweep_id, function=train_model, count=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
